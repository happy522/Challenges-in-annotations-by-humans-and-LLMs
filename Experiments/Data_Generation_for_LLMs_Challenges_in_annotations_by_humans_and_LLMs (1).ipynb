{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfIHNgVsLl9V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "from openai import OpenAI\n",
        "\n",
        "# ==========================\n",
        "# CONFIGURATION\n",
        "# ==========================\n",
        "\n",
        "BASE_URL = \"https://chat-ai.academiccloud.de/v1\"\n",
        "\n",
        "# Two API keys to rotate between when rate limits are hit.\n",
        "API_KEYS = [\n",
        "    \"d20572310bbacaf2823af34cecd24a42\",\n",
        "    \"03a2fb72225c23aa47c97742fe757224\"\n",
        "]\n",
        "\n",
        "if not any(API_KEYS):\n",
        "    raise RuntimeError(\"No API keys configured. Set ACAD_API_KEY_1 / ACAD_API_KEY_2 or put them in API_KEYS.\")\n",
        "\n",
        "current_key_index = 0\n",
        "\n",
        "MODEL = os.getenv(\"MODEL\", \"qwen3-30b-a3b-instruct-2507\")\n",
        "\n",
        "TEXT_COL = \"text\"  # column name containing texts\n",
        "\n",
        "# NEW: folders\n",
        "INPUT_DIR = \"input_excels\"       # folder with art5.xlsx, art6.xlsx, pol23.xlsx, ...\n",
        "OUTPUT_DIR = \"classified_excels\" # folder where classified files will be written\n",
        "\n",
        "PER_CALL_DELAY = 0.5\n",
        "BATCH_DELAY = 1.0\n",
        "\n",
        "# ==========================\n",
        "# CLIENT MANAGEMENT\n",
        "# ==========================\n",
        "\n",
        "def make_client():\n",
        "    key = API_KEYS[current_key_index]\n",
        "    if not key:\n",
        "        raise RuntimeError(f\"No API key configured at index {current_key_index}\")\n",
        "    return OpenAI(api_key=key, base_url=BASE_URL)\n",
        "\n",
        "client = make_client()\n",
        "\n",
        "def rotate_api_key():\n",
        "    global current_key_index, client\n",
        "    num_keys = len(API_KEYS)\n",
        "    if num_keys <= 1:\n",
        "        print(\"[INFO] Only one API key configured; cannot rotate.\")\n",
        "        return\n",
        "    current_key_index = (current_key_index + 1) % num_keys\n",
        "    client = make_client()\n",
        "    print(f\"[INFO] Switched to API key #{current_key_index + 1}\")\n",
        "\n",
        "# ==========================\n",
        "# PROMPTS\n",
        "# ==========================\n",
        "\n",
        "PROMPT_BINARY = \"\"\"\n",
        "You are an annotation assistant for Appraisal Theory (Martin & White, 2005).\n",
        "\n",
        "Task:\n",
        "Decide whether the TARGET text contains any evaluative content.\n",
        "\n",
        "You are given:\n",
        "- PREVIOUS: text from the row above (may be empty)\n",
        "- TARGET: the current text to label\n",
        "- NEXT: text from the row below (may be empty)\n",
        "\n",
        "Use PREVIOUS and NEXT only as context (e.g., to resolve pronouns or topics).\n",
        "The label must refer ONLY to the TARGET text.\n",
        "\n",
        "Coding rule “Evaluative” (binary):\n",
        "\n",
        "- Evaluative = 1\n",
        "  -> If the TARGET text expresses an emotion, a judgment of people/behaviour,\n",
        "     or an aesthetic/quality/value evaluation of things, performances or phenomena.\n",
        "\n",
        "- Evaluative = 0\n",
        "  -> If the TARGET text is purely factual, descriptive, or neutral, with no clear\n",
        "     emotional stance, no judgment of people/behaviour, and no quality/value\n",
        "     evaluation of things or events.\n",
        "\n",
        "Examples (illustrative only):\n",
        "- “The room is beautiful.” -> evaluative (1): aesthetic evaluation of a thing.\n",
        "- “She was very unfair to her team.” -> evaluative (1): judgment of behaviour.\n",
        "- “The concert made me really happy.” -> evaluative (1): emotional reaction.\n",
        "- “The meeting starts at 3 pm.” -> non-evaluative (0): purely factual.\n",
        "\n",
        "Important constraints:\n",
        "- Do NOT invent or assume information that is not explicitly present in the\n",
        "  TARGET text or necessary to interpret it in context. No hallucinations.\n",
        "- If you are unsure, choose the more conservative option and explain briefly.\n",
        "\n",
        "Output format:\n",
        "Return ONLY a single valid JSON object (no extra text, no explanations outside JSON).\n",
        "Use exactly these fields:\n",
        "\n",
        "- \"binary\": 0 or 1\n",
        "    - 0 = non-evaluative TARGET\n",
        "    - 1 = evaluative TARGET\n",
        "- \"label\": \"no_eval\" or \"eval\"\n",
        "- \"justification\": a short justification for the decision (max 40 words), based ONLY on the given texts.\n",
        "- \"evidence_span\": a short quote from the TARGET that supports the decision (or \"\" if none).\n",
        "\n",
        "Context:\n",
        "\n",
        "PREVIOUS:\n",
        "\\\"\\\"\\\"{prev_text}\\\"\\\"\\\"\n",
        "\n",
        "TARGET:\n",
        "\\\"\\\"\\\"{target_text}\\\"\\\"\\\"\n",
        "\n",
        "NEXT:\n",
        "\\\"\\\"\\\"{next_text}\\\"\\\"\\\"\"\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_MULTICLASS = \"\"\"You are an annotation assistant for Appraisal Theory (Martin & White, 2005).\n",
        "\n",
        "The TARGET text has been identified as evaluative (binary = 1).\n",
        "\n",
        "Task:\n",
        "Classify the TARGET text into Appraisal categories. Use the three main types:\n",
        "- Affect\n",
        "- Judgment\n",
        "- Appreciation\n",
        "\n",
        "You are given:\n",
        "- PREVIOUS: text from the row above (may be empty)\n",
        "- TARGET: the current text to label\n",
        "- NEXT: text from the row below (may be empty)\n",
        "\n",
        "Use PREVIOUS and NEXT only as context (e.g., to resolve referents).\n",
        "Multiple labels are allowed ONLY IF more than one label plausibly applies but none clearly dominates, mark Ambiguous = 1 and also mark the candidate labels.\n",
        "\n",
        "Definitions (paraphrased from Appraisal Theory):\n",
        "\n",
        "1. Affect (feelings / emotions)\n",
        "   - The TARGET text expresses emotional states or reactions.\n",
        "   - This can include:\n",
        "     - emotional reactions to events or things (e.g., “I’m happy about it”, “That scares me”),\n",
        "     - more enduring emotional dispositions (e.g., “I’m a nervous person”).\n",
        "   - Typical linguistic cues:\n",
        "     - emotion words (happy, sad, afraid, angry, delighted, bored, grateful, etc.),\n",
        "     - phrases like “makes me feel…”, “I love / hate…”.\n",
        "\n",
        "2. Judgment (evaluation of people and behaviour)\n",
        "   - The TARGET text evaluates people or their behaviour/character in relation to social norms\n",
        "     (e.g., right/wrong, fair/unfair, honest/dishonest, capable/incompetent).\n",
        "   - Often about:\n",
        "     - social esteem (e.g., brave, careful, talented, lazy, rude),\n",
        "     - social sanction (e.g., honest, trustworthy, corrupt, unfair, immoral).\n",
        "   - Typical linguistic cues:\n",
        "     - adjectives for character or behaviour (“kind”, “irresponsible”, “unfair”),\n",
        "     - attributions of praise or blame (“she did the right thing”, “he failed his duty”).\n",
        "\n",
        "3. Appreciation (evaluation of things, products, events, processes, phenomena)\n",
        "   - The TARGET text evaluates objects, artefacts, performances, texts, policies, situations,\n",
        "     or environments in terms of quality, value, design, or aesthetics.\n",
        "   - Often about:\n",
        "     - reaction (how appealing or impactful something is: “boring”, “exciting”, “moving”),\n",
        "     - composition (harmony, complexity, balance: “well-structured”, “chaotic”),\n",
        "     - valuation (worth, significance: “important”, “valuable”, “pointless”).\n",
        "   - Typical linguistic cues:\n",
        "     - evaluations of things or outcomes (“beautiful view”, “poor performance”, “excellent report”).\n",
        "\n",
        "4. Ambiguous\n",
        "   - Use “ambiguous” when:\n",
        "     - more than one type (Affect, Judgment, Appreciation) clearly applies,\n",
        "       AND none of them is clearly dominant, OR\n",
        "     - the available text is too short or vague to reliably decide which type it is.\n",
        "   - In such cases, mark the types you are considering with 1 (e.g. Affect = 1, Judgment = 1)\n",
        "     and set \"label\" to \"ambiguous\".\n",
        "\n",
        "Important constraints:\n",
        "- Base your decision ONLY on the given texts and the definitions above.\n",
        "- Do NOT invent extra information or background that is not in the context.\n",
        "- You can choose more than one type (e.g., Affect = 1 and Judgement = 1) – multi-label annotation is allowed.\n",
        "- If you genuinely cannot decide which category is primary, use \"ambiguous\" and mark the candidates.\n",
        "\n",
        "Output format:\n",
        "Return ONLY a single valid JSON object (no extra text).\n",
        "\n",
        "Use exactly these fields:\n",
        "- \"label\": one of \"affect\", \"judgment\", \"appreciation\", \"ambiguous\"\n",
        "- \"affect\": 0 or 1\n",
        "- \"judgment\": 0 or 1\n",
        "- \"appreciation\": 0 or 1\n",
        "- \"probability_affect\": float between 0.0 and 1.0\n",
        "- \"probability_judgment\": float between 0.0 and 1.0\n",
        "- \"probability_appreciation\": float between 0.0 and 1.0\n",
        "- \"top_spans\": list (max 3 items) of short quotes from TARGET\n",
        "- \"explanation\": short explanation (max 60 words)\n",
        "\n",
        "Context:\n",
        "\n",
        "PREVIOUS:\n",
        "\\\"\\\"\\\"{prev_text}\\\"\\\"\\\"\"\n",
        "\n",
        "TARGET:\n",
        "\\\"\\\"\\\"{target_text}\\\"\\\"\\\"\"\n",
        "\n",
        "NEXT:\n",
        "\\\"\\\"\\\"{next_text}\\\"\\\"\\\"\"\n",
        "\"\"\"\n",
        "\n",
        "# ==========================\n",
        "# MODEL CALL HELPER\n",
        "# ==========================\n",
        "\n",
        "def call_model(messages, model=MODEL, max_tokens=300, temperature=0.0, retries=5):\n",
        "    global client\n",
        "    last_exc = None\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "            )\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "\n",
        "            m = re.search(r'(\\{[\\s\\S]*\\})', content)\n",
        "            json_text = m.group(1) if m else content\n",
        "            parsed = json.loads(json_text)\n",
        "\n",
        "            time.sleep(PER_CALL_DELAY)\n",
        "            return parsed, content\n",
        "\n",
        "        except Exception as e:\n",
        "            msg = str(e)\n",
        "            is_rate_limit = (\"429\" in msg) or (\"rate limit\" in msg.lower())\n",
        "\n",
        "            if is_rate_limit:\n",
        "                print(f\"[WARN] Rate limit error: {e}\")\n",
        "                rotate_api_key()\n",
        "                wait = 5 * (attempt + 1)\n",
        "                print(f\"[INFO] Sleeping {wait} seconds before retry...\")\n",
        "                time.sleep(wait)\n",
        "                last_exc = e\n",
        "                continue\n",
        "            else:\n",
        "                wait = 2 ** attempt\n",
        "                print(f\"[ERROR] Non-rate-limit error: {e}. Sleeping {wait} seconds before retry...\")\n",
        "                time.sleep(wait)\n",
        "                last_exc = e\n",
        "                continue\n",
        "\n",
        "    raise last_exc\n",
        "\n",
        "# ==========================\n",
        "# MAIN PIPELINE FOR ONE FILE\n",
        "# ==========================\n",
        "\n",
        "def classify_excel(input_file, text_col=TEXT_COL, output_file=None):\n",
        "    \"\"\"\n",
        "    Classify a single Excel file and write the result to output_file.\n",
        "    If output_file is None, appends '_classified' before extension.\n",
        "    \"\"\"\n",
        "    if output_file is None:\n",
        "        base, ext = os.path.splitext(input_file)\n",
        "        output_file = base + \"_classified\" + ext\n",
        "\n",
        "    print(f\"[INFO] Classifying file: {input_file}\")\n",
        "    df = pd.read_excel(input_file)\n",
        "\n",
        "    df[\"binary\"] = None\n",
        "    df[\"binary_justification\"] = None\n",
        "    df[\"binary_evidence_span\"] = None\n",
        "    df[\"multiclass_label\"] = None\n",
        "    df[\"multiclass_probability_affect\"] = None\n",
        "    df[\"multiclass_probability_judgment\"] = None\n",
        "    df[\"multiclass_probability_appreciation\"] = None\n",
        "    df[\"multiclass_spans\"] = None\n",
        "    df[\"multiclass_explanation\"] = None\n",
        "    df[\"binary_raw_output\"] = None\n",
        "    df[\"multiclass_raw_output\"] = None\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text = str(row.get(text_col, \"\")).strip()\n",
        "\n",
        "        if not text:\n",
        "            df.at[idx, \"binary\"] = 0\n",
        "            df.at[idx, \"binary_justification\"] = \"leer\"\n",
        "            continue\n",
        "\n",
        "        prev_text = str(df.at[idx - 1, text_col]).strip() if idx > 0 else \"\"\n",
        "        next_text = str(df.at[idx + 1, text_col]).strip() if idx < len(df) - 1 else \"\"\n",
        "\n",
        "        # ---------- BINARY ----------\n",
        "        messages_bin = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a precise classifier of appraisal theory. Respond only with valid JSON.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": PROMPT_BINARY.format(\n",
        "                    prev_text=prev_text,\n",
        "                    target_text=text,\n",
        "                    next_text=next_text,\n",
        "                ),\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            parsed_bin, raw_bin = call_model(messages_bin)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Binary classification failed at idx={idx} in file {input_file}: {e}\")\n",
        "            df.at[idx, \"binary\"] = None\n",
        "            df.at[idx, \"binary_justification\"] = f\"error: {e}\"\n",
        "            df.at[idx, \"binary_raw_output\"] = \"\"\n",
        "            time.sleep(BATCH_DELAY)\n",
        "            continue\n",
        "\n",
        "        df.at[idx, \"binary_raw_output\"] = raw_bin\n",
        "\n",
        "        bin_val = parsed_bin.get(\"binary\")\n",
        "        try:\n",
        "            bin_val = int(bin_val)\n",
        "        except Exception:\n",
        "            label = str(parsed_bin.get(\"label\", \"\")).lower()\n",
        "            bin_val = 1 if \"eval\" in label else 0\n",
        "\n",
        "        df.at[idx, \"binary\"] = bin_val\n",
        "        df.at[idx, \"binary_justification\"] = parsed_bin.get(\"justification\", \"\")\n",
        "        df.at[idx, \"binary_evidence_span\"] = parsed_bin.get(\"evidence_span\", \"\")\n",
        "\n",
        "        # ---------- MULTICLASS ----------\n",
        "        if bin_val == 1:\n",
        "            messages_multi = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a precise classifier of appraisal theory. Respond only with valid JSON.\",\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": PROMPT_MULTICLASS.format(\n",
        "                        prev_text=prev_text,\n",
        "                        target_text=text,\n",
        "                        next_text=next_text,\n",
        "                    ),\n",
        "                },\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                parsed_multi, raw_multi = call_model(messages_multi)\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Multiclass classification failed at idx={idx} in file {input_file}: {e}\")\n",
        "                df.at[idx, \"multiclass_label\"] = None\n",
        "                df.at[idx, \"multiclass_explanation\"] = f\"error: {e}\"\n",
        "                df.at[idx, \"multiclass_raw_output\"] = \"\"\n",
        "                time.sleep(BATCH_DELAY)\n",
        "                continue\n",
        "\n",
        "            df.at[idx, \"multiclass_raw_output\"] = raw_multi\n",
        "            df.at[idx, \"multiclass_label\"] = parsed_multi.get(\"label\", None)\n",
        "\n",
        "            try:\n",
        "                df.at[idx, \"multiclass_probability_affect\"] = float(parsed_multi.get(\"probability_affect\", 0.0))\n",
        "            except Exception:\n",
        "                df.at[idx, \"multiclass_probability_affect\"] = None\n",
        "            try:\n",
        "                df.at[idx, \"multiclass_probability_judgment\"] = float(parsed_multi.get(\"probability_judgment\", 0.0))\n",
        "            except Exception:\n",
        "                df.at[idx, \"multiclass_probability_judgment\"] = None\n",
        "            try:\n",
        "                df.at[idx, \"multiclass_probability_appreciation\"] = float(parsed_multi.get(\"probability_appreciation\", 0.0))\n",
        "            except Exception:\n",
        "                df.at[idx, \"multiclass_probability_appreciation\"] = None\n",
        "\n",
        "            spans = parsed_multi.get(\"top_spans\", [])\n",
        "            if isinstance(spans, list):\n",
        "                df.at[idx, \"multiclass_spans\"] = \" ||| \".join(spans)\n",
        "            else:\n",
        "                df.at[idx, \"multiclass_spans\"] = spans\n",
        "\n",
        "            df.at[idx, \"multiclass_explanation\"] = parsed_multi.get(\"explanation\", \"\")\n",
        "\n",
        "        time.sleep(BATCH_DELAY)\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"[INFO] Saved classified Excel to {output_file}\")\n",
        "\n",
        "# ==========================\n",
        "# BATCH OVER A FOLDER\n",
        "# ==========================\n",
        "\n",
        "def classify_all_excels(input_dir=INPUT_DIR, output_dir=OUTPUT_DIR, text_col=TEXT_COL):\n",
        "    \"\"\"\n",
        "    Loop over all .xlsx files in input_dir and classify each one.\n",
        "    Output is written to output_dir with '<name>_classified.xlsx'.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    files = [f for f in os.listdir(input_dir)\n",
        "             if f.lower().endswith(\".xlsx\") and not f.startswith(\"~$\")]\n",
        "\n",
        "    if not files:\n",
        "        print(f\"[WARN] No .xlsx files found in {input_dir}\")\n",
        "        return\n",
        "\n",
        "    for fname in files:\n",
        "        input_path = os.path.join(input_dir, fname)\n",
        "        base, ext = os.path.splitext(fname)\n",
        "        output_path = os.path.join(output_dir, f\"{base}_classified{ext}\")\n",
        "\n",
        "        # Avoid re-processing if output already exists (optional)\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"[INFO] Skipping {fname}, already classified at {output_path}\")\n",
        "            continue\n",
        "\n",
        "        classify_excel(input_file=input_path, text_col=text_col, output_file=output_path)\n",
        "\n",
        "# ==========================\n",
        "# ENTRY POINT\n",
        "# ==========================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    classify_all_excels()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N-TmIC9llRyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}