=== BINARY CLASSIFICATION REPORT === (test_eval_binary.py)
                precision    recall  f1-score   support

Non-evaluative       0.70      0.45      0.55        71
    Evaluative       0.70      0.87      0.77       105

      accuracy                           0.70       176
     macro avg       0.70      0.66      0.66       176
  weighted avg       0.70      0.70      0.68       176

Accuracy : 0.6989
Precision: 0.7000
Recall   : 0.8667
F1       : 0.7745


=== PER-LABEL REPORT ===
              precision    recall  f1-score   support

   judgement       0.17      0.86      0.28        22
      affect       0.18      0.15      0.17        39
appreciation       0.30      1.00      0.47        39

   micro avg       0.23      0.64      0.34       100
   macro avg       0.22      0.67      0.30       100
weighted avg       0.23      0.64      0.31       100
 samples avg       0.23      0.44      0.30       100

MICRO     Precision=0.2319  Recall=0.6400  F1=0.3404
MACRO     Precision=0.2172  Recall=0.6725  F1=0.3037
WEIGHTED  Precision=0.2261  Recall=0.6400  F1=0.3082

Subset accuracy (exact match): 0.038461538461538464

Saved predictions to: ./qwen_appraisal_multilabel_lora/multilabel_evaluation_predictions.csv
